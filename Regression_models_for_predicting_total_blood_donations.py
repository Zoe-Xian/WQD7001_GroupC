# -*- coding: utf-8 -*-
"""modle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10otl0Rtom-Pe9ZSiVEqpQeqtf6XpVvdp

# WQD 7001 Data Wizards​

Regression models for predicting total blood donations

1.Prophet Model​
"""

import pandas as pd
import numpy as np
from scipy import stats
from prophet import Prophet
from prophet.diagnostics import cross_validation, performance_metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load the data
df = pd.read_csv('cleaned_donations_facility_new.csv')

# Extract the necessary columns
data = df[['date', 'daily']]

# Convert the 'date' column to datetime
data['ds'] = pd.to_datetime(data['date'])
data['y'] = data['daily']

# Split the data into train and test sets
test = data[data['ds'].dt.year == 2024]
train = data[data['ds'].dt.year < 2024]

# Handle outliers
z = np.abs(stats.zscore(train['y']))
train_cleaned = train[z < 3]

# Tune hyperparameters
model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)
model.fit(train_cleaned)

# Make predictions on the test set
future = model.make_future_dataframe(periods=len(test))
forecast = model.predict(future)

# Evaluate the model performance
test_predictions = forecast['yhat'][-len(test):]
rmse = np.sqrt(mean_squared_error(test['y'], test_predictions))
mae = mean_absolute_error(test['y'], test_predictions)
r2 = r2_score(test['y'], test_predictions)

# Calculate accuracy within 20% error
def within_percentage_error(y_true, y_pred, threshold=0.20):
    # Calculate percentage error for each prediction
    percentage_error = np.abs(y_true - y_pred) / y_true
    # Count predictions within threshold
    within_threshold = np.sum(percentage_error <= threshold)
    # Calculate accuracy
    accuracy = within_threshold / len(y_true)
    return accuracy

# Calculate accuracy within 20% error
accuracy_20 = within_percentage_error(test['y'].values, test_predictions.values)

# Print all metrics including the new accuracy metric
print(f'RMSE: {rmse:.2f}')
print(f'MAE: {mae:.2f}')
print(f'R-squared: {r2:.2f}')
print(f'Accuracy (within 20% error): {accuracy_20:.2f}')

"""2.LightGBM model ​"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('cleaned_donations_facility_new.csv')

# Extract and preprocess data
data = df[['date', 'daily']]
data['date'] = pd.to_datetime(data['date'])

# Create time-based features
data['year'] = data['date'].dt.year
data['month'] = data['date'].dt.month
data['day'] = data['date'].dt.day
data['dayofweek'] = data['date'].dt.dayofweek
data['quarter'] = data['date'].dt.quarter
data['is_weekend'] = data['dayofweek'].isin([5, 6]).astype(int)

# Create lag features
for i in [1, 2, 3, 7, 14, 30]:
    data[f'lag_{i}'] = data['daily'].shift(i)

# Create rolling mean features
for window in [7, 14, 30]:
    data[f'rolling_mean_{window}'] = data['daily'].rolling(window=window).mean()

# Split the data
test = data[data['year'] == 2024].copy()
train = data[data['year'] < 2024].copy()

# Remove rows with NaN values
train = train.dropna()

# Prepare features
feature_columns = ['month', 'day', 'dayofweek', 'quarter', 'is_weekend'] + \
                 [col for col in train.columns if col.startswith(('lag_', 'rolling_mean_'))]

X_train = train[feature_columns]
y_train = train['daily']

X_test = test[feature_columns]
y_test = test['daily']

# Train LightGBM model
model = LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    num_leaves=31,
    max_depth=-1,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Calculate metrics
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

def calculate_accuracy(y_true, y_pred, threshold=0.2):
    correct = np.sum(np.abs(y_true - y_pred) <= threshold * y_true)
    return correct / len(y_true)

accuracy = calculate_accuracy(y_test, predictions)

print(f'RMSE: {rmse:.2f}')
print(f'MAE: {mae:.2f}')
print(f'R-squared: {r2:.2f}')
print(f'Accuracy (within 20% error): {accuracy:.2f}')

# Visualization
plt.figure(figsize=(15, 8))
plt.plot(test['date'], y_test, label='Actual', alpha=0.7)
plt.plot(test['date'], predictions, label='Predicted', alpha=0.7)

plt.xlabel('Date')
plt.ylabel('Daily Blood Donations')
plt.title('Blood Donation Prediction for Malaysia: LightGBM Model')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Feature importance
importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': model.feature_importances_
})
importance = importance.sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.bar(importance['feature'][:10], importance['importance'][:10])
plt.xticks(rotation=45)
plt.title('Top 10 Most Important Features')
plt.tight_layout()
plt.show()

"""3.XGBoost model ​"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('cleaned_donations_facility_new.csv')

# Extract and preprocess data
data = df[['date', 'daily']]
data['date'] = pd.to_datetime(data['date'])

# Enhanced feature engineering
def create_features(df):
    df = df.copy()

    # Basic time features
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    df['dayofweek'] = df['date'].dt.dayofweek
    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

    # Lag features with outlier handling
    for i in [1, 2, 3, 7, 14]:
        df[f'lag_{i}'] = df['daily'].shift(i)

    # Rolling features with outlier handling
    for window in [7, 14, 30]:
        df[f'rolling_median_{window}'] = df['daily'].rolling(window=window).median()
        df[f'rolling_q75_{window}'] = df['daily'].rolling(window=window).quantile(0.75)
        df[f'rolling_q25_{window}'] = df['daily'].rolling(window=window).quantile(0.25)

    return df

# Create features
data = create_features(data)

# Split the data
test = data[data['date'].dt.year == 2024].copy()
train = data[data['date'].dt.year < 2024].copy()

# Handle outliers using IQR method
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
    return df

# Handle outliers in training data
train = handle_outliers(train.copy(), 'daily')

# Remove NaN values
train = train.dropna()
test = test.dropna()

# Prepare features
exclude_columns = ['date', 'daily']
feature_columns = [col for col in train.columns if col not in exclude_columns]

X_train = train[feature_columns]
y_train = train['daily']
X_test = test[feature_columns]
y_test = test['daily']

# Scale features using RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train XGBoost model
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.01,
    max_depth=6,
    min_child_weight=1,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.01,
    reg_lambda=1,
    random_state=42,
    early_stopping_rounds=50,
    verbose=0
)

# Train with early stopping
model.fit(
    X_train_scaled, y_train,
    eval_set=[(X_train_scaled, y_train)],
    verbose=0
)

# Make predictions
predictions = model.predict(X_test_scaled)

# Calculate metrics
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

def calculate_accuracy(y_true, y_pred, threshold=0.2):
    correct = np.sum(np.abs(y_true - y_pred) <= threshold * y_true)
    return correct / len(y_true)

accuracy = calculate_accuracy(y_test, predictions)

print(f'RMSE: {rmse:.2f}')
print(f'MAE: {mae:.2f}')
print(f'R-squared: {r2:.2f}')
print(f'Accuracy (within 20% error): {accuracy:.2f}')

# Visualization
plt.figure(figsize=(15, 8))
plt.plot(test['date'], y_test, label='Actual', alpha=0.7)
plt.plot(test['date'], predictions, label='Predicted', alpha=0.7)

plt.xlabel('Date')
plt.ylabel('Daily Blood Donations')
plt.title('Blood Donation Prediction for Malaysia: XGBoost Model')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Feature importance
importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': model.feature_importances_
})
importance = importance.sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.bar(importance['feature'][:10], importance['importance'][:10])
plt.xticks(rotation=45)
plt.title('Top 10 Most Important Features')
plt.tight_layout()
plt.show()

"""4.The best model:

Combining temporal features and XGBoost models
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit

# Load the data
df = pd.read_csv('cleaned_donations_facility_new.csv')
data = df[['date', 'daily']]
data['date'] = pd.to_datetime(data['date'])

def create_advanced_features(df):
    df = df.copy()

    # Basic Time Features
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    df['dayofweek'] = df['date'].dt.dayofweek
    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)
    df['week'] = df['date'].dt.isocalendar().week

    # Cyclic coding time features
    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)
    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)
    df['day_sin'] = np.sin(2 * np.pi * df['day']/31)
    df['day_cos'] = np.cos(2 * np.pi * df['day']/31)
    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)
    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)

    # Lagged features
    for i in [1, 2, 3, 7, 14]:
        df[f'lag_{i}'] = df['daily'].shift(i)

    # Rolling statistical features
    windows = [7, 14, 30]
    for window in windows:
        df[f'rolling_median_{window}'] = df['daily'].rolling(window=window).median()
        df[f'rolling_q25_{window}'] = df['daily'].rolling(window=window).quantile(0.25)
        df[f'rolling_q75_{window}'] = df['daily'].rolling(window=window).quantile(0.75)

    # Differential features
    df['diff_1'] = df['daily'].diff()
    df['diff_7'] = df['daily'].diff(7)

    # Replacement of infinity values
    df = df.replace([np.inf, -np.inf], np.nan)

    return df

# Creating Features
data = create_advanced_features(data)

# Split Data
test = data[data['date'].dt.year == 2024].copy()
train = data[data['date'].dt.year < 2024].copy()

# Outlier Handling
def handle_outliers_by_groups(df, target_col, group_cols):
    df = df.copy()
    for name, group in df.groupby(group_cols):
        Q1 = group[target_col].quantile(0.25)
        Q3 = group[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        group_idx = group.index
        # Displaying Converted Data Types
        clipped_values = df.loc[group_idx, target_col].clip(lower=lower_bound, upper=upper_bound).astype('int64')
        df.loc[group_idx, target_col] = clipped_values
    return df

# Handling of outliers grouped by month and day of the week
train = handle_outliers_by_groups(train, 'daily', ['month', 'dayofweek'])

# Filling in missing values
train = train.fillna(method='ffill').fillna(method='bfill')
test = test.fillna(method='ffill').fillna(method='bfill')

# Preparing features
exclude_columns = ['date', 'daily']
feature_columns = [col for col in train.columns if col not in exclude_columns]

X_train = train[feature_columns]
y_train = train['daily']
X_test = test[feature_columns]
y_test = test['daily']

# Feature Scaling with RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Creating and training XGBoost models
model = XGBRegressor(
    n_estimators=2000,
    learning_rate=0.005,
    max_depth=6,
    min_child_weight=2,
    gamma=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1,
    random_state=42
)

# Training the model
model.fit(X_train_scaled, y_train)

# Make prediction
predictions = model.predict(X_test_scaled)

# Calculate evaluation metrics
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

def calculate_accuracy(y_true, y_pred, threshold=0.2):
    correct = np.sum(np.abs(y_true - y_pred) <= threshold * y_true)
    return correct / len(y_true)

accuracy = calculate_accuracy(y_test, predictions)

print(f'RMSE: {rmse:.2f}')
print(f'MAE: {mae:.2f}')
print(f'R-squared: {r2:.2f}')
print(f'Accuracy (within 20% error): {accuracy:.2f}')

# Model Prediction Visualization
plt.figure(figsize=(15, 8))
plt.plot(test['date'], y_test, label='Actual', alpha=0.7)
plt.plot(test['date'], predictions, label='Predicted', alpha=0.7)

plt.xlabel('Date')
plt.ylabel('Daily Blood Donations')
plt.title('Blood Donation Prediction for Malaysia: Enhanced XGBoost Model')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Feature Importance Visualization
importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': model.feature_importances_
})
importance = importance.sort_values('importance', ascending=True)  # Sort in ascending order
importance = importance.tail(15)  # Get the last 15 (most important) features

plt.figure(figsize=(8, 10))
plt.barh(range(len(importance)), importance['importance'])  # Use numeric index to control position
plt.yticks(range(len(importance)), importance['feature'])  # et y-axis label
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Top 15 Most Important Features')
plt.tight_layout()
plt.show()
# Feature Importance Visualization
#importance = pd.DataFrame({
#    'feature': feature_columns,
#    'importance': model.feature_importances_
#})
#importance = importance.sort_values('importance', ascending=False)

#plt.figure(figsize=(12, 6))
#plt.bar(importance['feature'][:15], importance['importance'][:15])
#plt.xticks(rotation=45)
#plt.title('Top 15 Most Important Features')
#plt.tight_layout()
#plt.show()

"""Forecast of total blood donations in 2025:"""

import pandas as pd
import numpy as np
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler

# Load the data
df = pd.read_csv('cleaned_donations_facility_new.csv')
data = df[['date', 'daily']]
data['date'] = pd.to_datetime(data['date'])

# Excluding data for 2014
data = data[data['date'].dt.year > 2014]

# Calculation of historical statistics
yearly_stats = data.groupby(data['date'].dt.year)['daily'].agg(['mean', 'std', 'sum'])
monthly_stats = data.groupby(data['date'].dt.month)['daily'].agg(['mean', 'std', 'sum'])

print("\n Historical Annual Statistics:")
print(yearly_stats)

def create_features(df, historical_data=None):
    df = df.copy()

    # Underlying time characteristics
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    df['dayofweek'] = df['date'].dt.dayofweek
    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

    # periodic coding
    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)
    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)

    if historical_data is not None:
        # Monthly averages using historical data
        monthly_means = historical_data.groupby('month')['daily'].mean()
        df['monthly_mean'] = df['month'].map(monthly_means)

        # Rolling statistics using historical data
        for window in [7, 14, 30]:
            df[f'rolling_mean_{window}'] = monthly_means.mean()
            df[f'rolling_std_{window}'] = historical_data['daily'].std()
    else:
        for window in [7, 14, 30]:
            df[f'rolling_mean_{window}'] = df['daily'].rolling(window=window).mean()
            df[f'rolling_std_{window}'] = df['daily'].rolling(window=window).std()
        df['monthly_mean'] = df.groupby('month')['daily'].transform('mean')

    return df

# Split data
test = data[data['date'].dt.year == 2024].copy()
train = data[data['date'].dt.year < 2024].copy()

# Creating Features
train = create_features(train)
test = create_features(test)

# Filling in missing values
train = train.fillna(method='ffill').fillna(method='bfill')
test = test.fillna(method='ffill').fillna(method='bfill')

# Preparation features
exclude_columns = ['date', 'daily']
feature_columns = [col for col in train.columns if col not in exclude_columns]

X_train = train[feature_columns]
y_train = train['daily']
X_test = test[feature_columns]
y_test = test['daily']

# Create and train the model
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    random_state=42
)

model.fit(X_train, y_train)

# Creating projected data for 2025
future_dates = pd.date_range(start='2025-01-01', end='2025-12-31', freq='D')
future_df = pd.DataFrame({'date': future_dates})

# Creating a 2025 identity
future_df = create_features(future_df, historical_data=train)
future_features = future_df[feature_columns]

# Base forecast
base_predictions = model.predict(future_features)

# Adjustment of forecasts to match historical levels
target_daily_mean = yearly_stats['mean'].mean()
target_daily_std = yearly_stats['std'].mean()

# Generate monthly target totals (based on historical data)
monthly_targets = {}
for month in range(1, 13):
    historical_month_mean = monthly_stats.loc[month, 'mean']
    days_in_month = len(future_df[future_df['month'] == month])
    monthly_targets[month] = historical_month_mean * days_in_month

# Adjustment of projections by month
future_predictions = []
for month in range(1, 13):
    month_mask = future_df['month'] == month
    month_predictions = base_predictions[month_mask]

    # Calculation of monthly scaling factor
    target_month_total = monthly_targets[month]
    current_month_total = month_predictions.sum()
    scaling_factor = target_month_total / current_month_total

    # Apply scaling and add random fluctuations
    adjusted_predictions = month_predictions * scaling_factor

    # Adding random fluctuations
    np.random.seed(42 + month)  # Use different random seeds for different months
    daily_variation = np.random.normal(0, target_daily_std * 0.1, len(adjusted_predictions))
    adjusted_predictions += daily_variation

    # Ensure that projections are within reasonable limits
    min_daily = max(10, target_daily_mean * 0.3)  # Setting the minimum value
    max_daily = target_daily_mean * 2.0  # Setting the maximum value
    adjusted_predictions = np.clip(adjusted_predictions, min_daily, max_daily)

    future_predictions.extend(adjusted_predictions)

future_predictions = np.array(future_predictions)

# Final adjustments to ensure total annual
target_annual = yearly_stats['sum'].mean()
final_scaling_factor = target_annual / future_predictions.sum()
future_predictions = future_predictions * final_scaling_factor

# Aggregate monthly forecasts
monthly_predictions = pd.DataFrame()
monthly_predictions['monthly_total'] = [future_predictions[future_df['month'] == m].sum() for m in range(1, 13)]
monthly_predictions['monthly_mean'] = [future_predictions[future_df['month'] == m].mean() for m in range(1, 13)]
monthly_predictions['monthly_std'] = [future_predictions[future_df['month'] == m].std() for m in range(1, 13)]

monthly_predictions.index = ['January', 'February', 'March', 'April', 'May', 'June',
                           'July', 'August', 'September', 'October', 'November', 'December']

print("\n2025 Monthly Forecast Statistics:")
print(monthly_predictions.round(2))

print(f"\nTotal predicted blood donations in 2025:{future_predictions.sum():,.2f}")
print(f"2025 average daily predicted blood donations:{future_predictions.mean():.2f}")
print(f"2025 predicted standard deviation:{future_predictions.std():.2f}")

# visualization
plt.figure(figsize=(15, 8))
plt.plot(monthly_predictions.index, monthly_predictions['monthly_total'], marker='o')
plt.title('Forecast of monthly blood donations in 2025')
plt.xlabel('Months')
plt.ylabel('Monthly Blood Donation')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Daily data distribution
plt.figure(figsize=(15, 6))
plt.boxplot([future_predictions[future_df['month'] == m] for m in range(1, 13)],
            labels=monthly_predictions.index)
plt.title('Distribution of monthly blood donations in 2025')
plt.xlabel('Months')
plt.ylabel('Daily blood donation')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()